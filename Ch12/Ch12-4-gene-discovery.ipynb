{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d7dcb-3436-4438-8779-bdce87de77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch12-4 Gene Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff875f62-1114-4df1-8550-e25e5e59fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install biopython pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96b6cb-a229-4b83-803e-2645aba82180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Subtractive Comparative Genomics Pipeline\n",
    "\n",
    "This script demonstrates a workflow for identifying unique genes in a target organism\n",
    "by comparing against reference genomes. The pipeline includes:\n",
    "1. Genome sequence retrieval\n",
    "2. Gene prediction\n",
    "3. Sequence similarity comparison\n",
    "4. Filtering for unique genes\n",
    "5. Functional annotation of candidate genes\n",
    "\n",
    "Requirements:\n",
    "- Biopython\n",
    "- BLAST+ command line tools\n",
    "- Prodigal (for prokaryotic gene prediction)\n",
    "- HMMER and Pfam database (for functional annotation)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26edb33-940a-4cdd-a979-09204529f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import subprocess\n",
    "from Bio import SeqIO\n",
    "from Bio.Blast.Applications import NcbiblastpCommandline\n",
    "import pandas as pd\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b1214-b9ae-411d-98b4-1452d0fd1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Entrez\n",
    "from Bio import Entrez\n",
    "Entrez.email = \"your.email@example.com\"  # you can replace this with your email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6bbca-eb08-4348-a810-6fd9340dc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories for the pipeline\"\"\"\n",
    "    directories = ['genomes', 'predictions', 'blast_results', 'unique_genes', 'annotations']\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    return directories\n",
    "\n",
    "def download_genome(accession, output_dir):\n",
    "    \"\"\"Download genome from NCBI using accession number\"\"\"\n",
    "    output_file = f\"{output_dir}/{accession}.fasta\"\n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"Downloading {accession}...\")\n",
    "        # We use Entrez from Biopython to download the genomes\n",
    "        handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"fasta\", retmode=\"text\")\n",
    "        with open(output_file, 'w') as out_f:\n",
    "            out_f.write(handle.read())\n",
    "        print(f\"Downloaded {accession} to {output_file}\")\n",
    "    else:\n",
    "        print(f\"Genome {accession} already exists at {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def predict_genes(genome_file, output_dir, organism_type=\"prokaryote\"):\n",
    "    \"\"\"Predict genes from genome sequence\"\"\"\n",
    "    genome_name = os.path.basename(genome_file).split('.')[0]\n",
    "    output_prefix = f\"{output_dir}/{genome_name}\"\n",
    "    protein_file = f\"{output_prefix}_proteins.faa\"\n",
    "    \n",
    "    if not os.path.exists(protein_file):\n",
    "        print(f\"Predicting genes for {genome_name}...\")\n",
    "        if organism_type == \"prokaryote\":\n",
    "            # Using Prodigal for prokaryotic gene prediction\n",
    "            cmd = f\"prodigal -i {genome_file} -a {protein_file} -o {output_prefix}_genes.gff -f gff\"\n",
    "            subprocess.run(cmd, shell=True, check=True)\n",
    "        else:\n",
    "            # For eukaryotes, more complex gene prediction tools would be used\n",
    "            # This is simplified for demonstration\n",
    "            print(\"Eukaryotic gene prediction requires tools like Augustus or MAKER\")\n",
    "        print(f\"Gene prediction complete for {genome_name}\")\n",
    "    else:\n",
    "        print(f\"Gene predictions already exist for {genome_name}\")\n",
    "    \n",
    "    return protein_file\n",
    "\n",
    "def create_blast_database(protein_file):\n",
    "    \"\"\"Create BLAST database from protein sequences\"\"\"\n",
    "    db_name = protein_file\n",
    "    cmd = f\"makeblastdb -in {protein_file} -dbtype prot -out {db_name}\"\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "    return db_name\n",
    "\n",
    "def run_blast_comparison(query_proteins, subject_db, output_file, evalue=1e-5):\n",
    "    \"\"\"Run BLAST to compare query proteins against subject database\"\"\"\n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"Running BLAST comparison: {query_proteins} vs {subject_db}\")\n",
    "        blastp_cline = NcbiblastpCommandline(\n",
    "            query=query_proteins,\n",
    "            db=subject_db,\n",
    "            out=output_file,\n",
    "            outfmt=\"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\",\n",
    "            evalue=evalue,\n",
    "            max_target_seqs=1\n",
    "        )\n",
    "        stdout, stderr = blastp_cline()\n",
    "        print(f\"BLAST comparison complete. Results saved to {output_file}\")\n",
    "    else:\n",
    "        print(f\"BLAST results already exist at {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def identify_unique_genes(target_protein_file, blast_results_files, output_dir, identity_threshold=30, coverage_threshold=50):\n",
    "    \"\"\"Identify genes in target organism that have no significant hits in reference organisms\"\"\"\n",
    "    target_name = os.path.basename(target_protein_file).split('_')[0]\n",
    "    \n",
    "    # Create a dictionary of all proteins in the target organism\n",
    "    target_proteins = {}\n",
    "    for record in SeqIO.parse(target_protein_file, \"fasta\"):\n",
    "        target_proteins[record.id] = record\n",
    "    \n",
    "    # Track proteins with significant hits in reference genomes\n",
    "    proteins_with_hits = set()\n",
    "    \n",
    "    # Process each BLAST result file\n",
    "    for blast_file in blast_results_files:\n",
    "        with open(blast_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                query_id = parts[0]\n",
    "                identity = float(parts[2])\n",
    "                alignment_length = float(parts[3])\n",
    "                query_length = len(target_proteins[query_id].seq)\n",
    "                coverage = (alignment_length / query_length) * 100\n",
    "                \n",
    "                # If a protein has a significant hit, add it to the set\n",
    "                if identity >= identity_threshold and coverage >= coverage_threshold:\n",
    "                    proteins_with_hits.add(query_id)\n",
    "    \n",
    "    # Identify proteins without significant hits (unique genes)\n",
    "    unique_proteins = set(target_proteins.keys()) - proteins_with_hits\n",
    "    \n",
    "    # Write unique proteins to output file\n",
    "    output_file = f\"{output_dir}/{target_name}_unique_proteins.faa\"\n",
    "    with open(output_file, 'w') as out_f:\n",
    "        for protein_id in unique_proteins:\n",
    "            SeqIO.write(target_proteins[protein_id], out_f, \"fasta\")\n",
    "    \n",
    "    print(f\"Identified {len(unique_proteins)} unique proteins in {target_name}\")\n",
    "    print(f\"Unique proteins saved to {output_file}\")\n",
    "    \n",
    "    return output_file, unique_proteins\n",
    "\n",
    "def annotate_unique_genes(unique_proteins_file, output_dir):\n",
    "    \"\"\"Annotate unique genes using HMMER and Pfam database\"\"\"\n",
    "    output_name = os.path.basename(unique_proteins_file).split('_')[0]\n",
    "    hmmer_output = f\"{output_dir}/{output_name}_pfam_annotations.txt\"\n",
    "    \n",
    "    # In a real implementation, run HMMER against Pfam database\n",
    "    cmd = f\"hmmscan --domtblout {hmmer_output} pfam/Pfam-A.hmm {unique_proteins_file}\"\n",
    "    # subprocess.run(cmd, shell=True, check=True)\n",
    "    \n",
    "    # This is a placeholder for parsing HMMER output\n",
    "    # In a real implementation, parse the domtblout file to extract domain annotations\n",
    "    annotations = {}\n",
    "    print(f\"Functional annotation complete. Results saved to {hmmer_output}\")\n",
    "    \n",
    "    return hmmer_output, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbb702-e488-4f69-bcc8-7729fd558f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Example usage with Mycobacterium tuberculosis as target and related species as references\n",
    "    \n",
    "    # 1. Set up directories\n",
    "    directories = setup_directories()\n",
    "    \n",
    "    # 2. Define target and reference organisms\n",
    "    target_accession = \"NC_000962\"  # M. tuberculosis H37Rv\n",
    "    reference_accessions = [\n",
    "        \"NC_002945\",  # M. bovis\n",
    "        \"NC_008769\",  # M. avium\n",
    "        \"NC_002677\"   # M. leprae\n",
    "    ]\n",
    "    \n",
    "    # 3. Download genomes\n",
    "    target_genome = download_genome(target_accession, directories[0])\n",
    "    reference_genomes = [download_genome(acc, directories[0]) for acc in reference_accessions]\n",
    "    \n",
    "    # 4. Predict genes\n",
    "    target_proteins = predict_genes(target_genome, directories[1])\n",
    "    reference_proteins = [predict_genes(genome, directories[1]) for genome in reference_genomes]\n",
    "    \n",
    "    # 5. Create BLAST databases for reference genomes\n",
    "    reference_dbs = [create_blast_database(proteins) for proteins in reference_proteins]\n",
    "    \n",
    "    # 6. Run BLAST comparisons\n",
    "    blast_results = []\n",
    "    for i, db in enumerate(reference_dbs):\n",
    "        ref_name = os.path.basename(reference_genomes[i]).split('.')[0]\n",
    "        target_name = os.path.basename(target_genome).split('.')[0]\n",
    "        output_file = f\"{directories[2]}/{target_name}_vs_{ref_name}.blast\"\n",
    "        blast_results.append(run_blast_comparison(target_proteins, db, output_file))\n",
    "    \n",
    "    # 7. Identify unique genes\n",
    "    unique_genes_file, unique_gene_ids = identify_unique_genes(\n",
    "        target_proteins, blast_results, directories[3]\n",
    "    )\n",
    "    \n",
    "    # 8. Annotate unique genes\n",
    "    annotation_file, annotations = annotate_unique_genes(unique_genes_file, directories[4])\n",
    "    \n",
    "    # 9. Generate summary report\n",
    "    print(\"\\nSubtractive Comparative Genomics Results:\")\n",
    "    print(f\"Target organism: {target_accession}\")\n",
    "    print(f\"Reference organisms: {', '.join(reference_accessions)}\")\n",
    "    print(f\"Total predicted proteins in target: {len(list(SeqIO.parse(target_proteins, 'fasta')))}\")\n",
    "    print(f\"Number of unique proteins identified: {len(unique_gene_ids)}\")\n",
    "    print(f\"Unique protein sequences saved to: {unique_genes_file}\")\n",
    "    print(f\"Functional annotations saved to: {annotation_file}\")\n",
    "    \n",
    "    # In a complete implementation, additional analyses could include:\n",
    "    # - GO term enrichment\n",
    "    # - Pathway analysis\n",
    "    # - Structural prediction\n",
    "    # - Phylogenetic analysis of unique genes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c504bc9-081a-4afe-9497-5e0d7b1d5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AI Tip - Prompt: Update code to actually run HMMER ##\n",
    "# This was done using Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6ba70-a59a-42ca-a447-456974464f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First - Download PFAM database\n",
    "# Create a directory for Pfam\n",
    "! mkdir -p ~/pfam\n",
    "\n",
    "# Download the Pfam database (this is a large file, ~1-2GB)\n",
    "! wget ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz -P ~/pfam\n",
    "\n",
    "# Uncompress the file\n",
    "! gunzip ~/pfam/Pfam-A.hmm.gz\n",
    "\n",
    "# Press (index) the database for HMMER\n",
    "! hmmpress ~/pfam/Pfam-A.hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0202e3-5807-4abd-a0d3-018efc45ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to update the annotation system to actually run HMMER against the PFAM database\n",
    "#. This code also stores the output in files, parses the results, and displays a visualization\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import logging\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def annotate_unique_genes(unique_proteins_file, output_dir, pfam_db=None, cpu=4, evalue=1e-5):\n",
    "    \"\"\"\n",
    "    Annotate unique genes using HMMER and Pfam database\n",
    "    \n",
    "    Parameters:\n",
    "        unique_proteins_file (str): Path to FASTA file with protein sequences\n",
    "        output_dir (str): Directory to save results\n",
    "        pfam_db (str): Path to Pfam-A.hmm database (if None, tries to locate it)\n",
    "        cpu (int): Number of CPU cores to use\n",
    "        evalue (float): E-value threshold for significance\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Path to HMMER output file, dictionary of parsed annotations)\n",
    "    \"\"\"\n",
    "    # Setup logging\n",
    "    logging.basicConfig(level=logging.INFO, \n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get base name for output files\n",
    "    output_name = os.path.basename(unique_proteins_file).split('_')[0]\n",
    "    hmmer_output = f\"{output_dir}/{output_name}_pfam_annotations.txt\"\n",
    "    parsed_output = f\"{output_dir}/{output_name}_pfam_parsed.tsv\"\n",
    "    \n",
    "    # Check if HMMER is installed\n",
    "    if not shutil.which(\"hmmscan\"):\n",
    "        logger.error(\"HMMER is not installed or not in PATH. Please install HMMER.\")\n",
    "        raise EnvironmentError(\"HMMER not found in PATH\")\n",
    "    \n",
    "    # Find Pfam database if not provided\n",
    "    if pfam_db is None:\n",
    "        # Common locations for Pfam database\n",
    "        possible_locations = [\n",
    "            \"/usr/local/share/pfam/Pfam-A.hmm\",\n",
    "            \"/usr/share/pfam/Pfam-A.hmm\",\n",
    "            os.path.expanduser(\"~/pfam/Pfam-A.hmm\"),\n",
    "            \"pfam/Pfam-A.hmm\"\n",
    "        ]\n",
    "        \n",
    "        for loc in possible_locations:\n",
    "            if os.path.exists(loc):\n",
    "                pfam_db = loc\n",
    "                logger.info(f\"Found Pfam database at {pfam_db}\")\n",
    "                break\n",
    "                \n",
    "        if pfam_db is None:\n",
    "            logger.error(\"Pfam database not found. Please specify the path to Pfam-A.hmm\")\n",
    "            raise FileNotFoundError(\"Pfam database not found\")\n",
    "    \n",
    "    # Check if Pfam database is pressed (indexed)\n",
    "    if not os.path.exists(f\"{pfam_db}.h3f\"):\n",
    "        logger.info(\"Pfam database needs to be pressed (indexed) first\")\n",
    "        press_cmd = f\"hmmpress {pfam_db}\"\n",
    "        try:\n",
    "            logger.info(f\"Running: {press_cmd}\")\n",
    "            subprocess.run(press_cmd, shell=True, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.error(f\"Failed to index Pfam database: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Run HMMER against Pfam database\n",
    "    cmd = f\"hmmscan --domtblout {hmmer_output} -E {evalue} --cpu {cpu} {pfam_db} {unique_proteins_file}\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Running HMMER: {cmd}\")\n",
    "        process = subprocess.run(cmd, shell=True, check=True, \n",
    "                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "                                universal_newlines=True)\n",
    "        logger.info(\"HMMER search completed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(f\"HMMER search failed: {e}\")\n",
    "        logger.error(f\"STDERR: {e.stderr}\")\n",
    "        raise\n",
    "    \n",
    "    # Parse HMMER output (domtblout format)\n",
    "    annotations = parse_hmmer_output(hmmer_output, parsed_output, evalue)\n",
    "    \n",
    "    logger.info(f\"Functional annotation complete. Results saved to {hmmer_output}\")\n",
    "    logger.info(f\"Parsed results saved to {parsed_output}\")\n",
    "    \n",
    "    return hmmer_output, annotations\n",
    "\n",
    "def parse_hmmer_output(hmmer_file, output_file, evalue_threshold=1e-5):\n",
    "    \"\"\"\n",
    "    Parse HMMER domtblout file and extract domain annotations\n",
    "    \n",
    "    Parameters:\n",
    "        hmmer_file (str): Path to HMMER domtblout output file\n",
    "        output_file (str): Path to save parsed results\n",
    "        evalue_threshold (float): E-value threshold for filtering\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with protein IDs as keys and lists of domain annotations as values\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    annotations = {}\n",
    "    \n",
    "    # Check if file exists and has content\n",
    "    if not os.path.exists(hmmer_file) or os.path.getsize(hmmer_file) == 0:\n",
    "        print(f\"Warning: HMMER output file {hmmer_file} is empty or doesn't exist\")\n",
    "        return annotations\n",
    "    \n",
    "    # Check file content and print first few lines\n",
    "    print(f\"\\nExamining HMMER output file: {hmmer_file}\")\n",
    "    with open(hmmer_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        data_lines = [line for line in lines if not line.startswith('#')]\n",
    "        \n",
    "        print(f\"Total lines: {len(lines)}\")\n",
    "        print(f\"Data lines (non-comment): {len(data_lines)}\")\n",
    "        \n",
    "        if len(data_lines) > 0:\n",
    "            print(\"First data line sample:\")\n",
    "            print(data_lines[0])\n",
    "    \n",
    "    # Parse the file\n",
    "    results = []\n",
    "    line_count = 0\n",
    "    parsed_count = 0\n",
    "    \n",
    "    with open(hmmer_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Skip comment lines\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "                \n",
    "            line_count += 1\n",
    "            \n",
    "            # Split the line on whitespace\n",
    "            # HMMER domtblout format has fixed columns but variable spacing\n",
    "            parts = line.strip().split()\n",
    "            \n",
    "            # Ensure we have enough parts for the required fields\n",
    "            if len(parts) < 22:  # Minimum expected fields\n",
    "                print(f\"Warning: Line has fewer fields than expected: {len(parts)}\")\n",
    "                print(f\"Line: {line.strip()}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Construct a result with key fields\n",
    "                # Note: domtblout format has target (hmm) first, then query (sequence)\n",
    "                # Get target info (HMM)\n",
    "                target_name = parts[0]\n",
    "                target_accession = parts[1]\n",
    "                \n",
    "                # Get query info (sequence)\n",
    "                query_name = parts[3]\n",
    "                \n",
    "                # Get e-values and scores\n",
    "                evalue_fullseq = float(parts[6])\n",
    "                score_fullseq = float(parts[7])\n",
    "                evalue_dom = float(parts[11])\n",
    "                score_dom = float(parts[12])\n",
    "                \n",
    "                # Get alignment positions\n",
    "                hmm_from = int(parts[15])\n",
    "                hmm_to = int(parts[16])\n",
    "                ali_from = int(parts[17])\n",
    "                ali_to = int(parts[18])\n",
    "                \n",
    "                # Get description (may be empty or multiple parts)\n",
    "                description = \" \".join(parts[22:]) if len(parts) >= 23 else \"\"\n",
    "                \n",
    "                # Filter by E-value\n",
    "                if evalue_dom <= evalue_threshold:\n",
    "                    row = {\n",
    "                        'target_name': target_name,\n",
    "                        'target_accession': target_accession,\n",
    "                        'query_name': query_name,\n",
    "                        'evalue_fullseq': evalue_fullseq,\n",
    "                        'score_fullseq': score_fullseq,\n",
    "                        'evalue_dom': evalue_dom,\n",
    "                        'score_dom': score_dom,\n",
    "                        'hmm_from': hmm_from,\n",
    "                        'hmm_to': hmm_to,\n",
    "                        'ali_from': ali_from,\n",
    "                        'ali_to': ali_to,\n",
    "                        'description': description\n",
    "                    }\n",
    "                    \n",
    "                    results.append(row)\n",
    "                    parsed_count += 1\n",
    "                    \n",
    "                    # Add to annotations dictionary\n",
    "                    if query_name not in annotations:\n",
    "                        annotations[query_name] = []\n",
    "                    \n",
    "                    domain_info = {\n",
    "                        'domain': target_name,\n",
    "                        'accession': target_accession,\n",
    "                        'evalue': evalue_dom,\n",
    "                        'score': score_dom,\n",
    "                        'start': ali_from,\n",
    "                        'end': ali_to,\n",
    "                        'description': description\n",
    "                    }\n",
    "                    \n",
    "                    annotations[query_name].append(domain_info)\n",
    "                \n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error parsing line: {line.strip()}\")\n",
    "                print(f\"Error details: {str(e)}\")\n",
    "    \n",
    "    print(f\"Processed {line_count} data lines\")\n",
    "    print(f\"Successfully parsed {parsed_count} domain hits\")\n",
    "    print(f\"Found annotations for {len(annotations)} proteins\")\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        df = df.sort_values(by=['query_name', 'evalue_dom'])\n",
    "        df.to_csv(output_file, sep='\\t', index=False)\n",
    "        print(f\"Saved parsed results to {output_file}\")\n",
    "    else:\n",
    "        print(\"No results to save to CSV\")\n",
    "        # Create empty file with headers\n",
    "        with open(output_file, 'w') as f:\n",
    "            headers = ['target_name', 'target_accession', 'query_name', \n",
    "                      'evalue_fullseq', 'score_fullseq', 'evalue_dom', \n",
    "                      'score_dom', 'hmm_from', 'hmm_to', 'ali_from', \n",
    "                      'ali_to', 'description']\n",
    "            f.write('\\t'.join(headers) + '\\n')\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "def summarize_annotations(annotations, output_dir, protein_file):\n",
    "    \"\"\"\n",
    "    Create a summary of domain annotations and visualize results\n",
    "    \n",
    "    Parameters:\n",
    "        annotations (dict): Dictionary of annotations from parse_hmmer_output\n",
    "        output_dir (str): Directory to save results\n",
    "        protein_file (str): Path to the original protein FASTA file\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to summary file\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    \n",
    "    output_name = os.path.basename(protein_file).split('_')[0]\n",
    "    summary_file = f\"{output_dir}/{output_name}_annotation_summary.tsv\"\n",
    "    \n",
    "    # Count domains\n",
    "    all_domains = []\n",
    "    for protein, domains in annotations.items():\n",
    "        for domain in domains:\n",
    "            all_domains.append(domain['domain'])\n",
    "    \n",
    "    domain_counts = Counter(all_domains)\n",
    "    \n",
    "    # Count proteins with annotations\n",
    "    total_proteins = len(list(SeqIO.parse(protein_file, \"fasta\")))\n",
    "    annotated_proteins = len(annotations)\n",
    "    annotation_rate = (annotated_proteins / total_proteins) * 100 if total_proteins > 0 else 0\n",
    "    \n",
    "    # Write summary\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"Total proteins analyzed: {total_proteins}\\n\")\n",
    "        f.write(f\"Proteins with at least one domain: {annotated_proteins} ({annotation_rate:.1f}%)\\n\")\n",
    "        f.write(f\"Total domains found: {len(all_domains)}\\n\")\n",
    "        f.write(f\"Unique domain types: {len(domain_counts)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Top 20 domains by frequency:\\n\")\n",
    "        for domain, count in domain_counts.most_common(20):\n",
    "            f.write(f\"{domain}\\t{count}\\n\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot top 15 domains\n",
    "    top_domains = dict(domain_counts.most_common(15))\n",
    "    plt.bar(top_domains.keys(), top_domains.values())\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"Top 15 Protein Domains in {output_name}\")\n",
    "    plt.xlabel(\"Domain\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f\"{output_dir}/{output_name}_domain_distribution.png\", dpi=300)\n",
    "    \n",
    "    return summary_file\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set file paths\n",
    "    protein_file = \"unique_genes/NC_unique_proteins.faa\". # this file was produced from your previous comparative genomics run\n",
    "    output_dir = \"annotation_results\"\n",
    "    \n",
    "    # Check if the protein file exists\n",
    "    if not os.path.exists(protein_file):\n",
    "        print(f\"ERROR: Protein file not found at {protein_file}\")\n",
    "        print(\"Please make sure the file exists at this location before running.\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(f\"Found protein file: {protein_file}\")\n",
    "    \n",
    "    # Validate and analyze the protein sequences\n",
    "    try:\n",
    "        sequences = list(SeqIO.parse(protein_file, \"fasta\"))\n",
    "        seq_count = len(sequences)\n",
    "        \n",
    "        if seq_count == 0:\n",
    "            print(\"ERROR: No valid sequences found in the file.\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        print(f\"Found {seq_count} sequences in {protein_file}\")\n",
    "        \n",
    "        # Calculate sequence statistics\n",
    "        seq_lengths = [len(record.seq) for record in sequences]\n",
    "        avg_length = sum(seq_lengths) / len(seq_lengths)\n",
    "        min_length = min(seq_lengths)\n",
    "        max_length = max(seq_lengths)\n",
    "        \n",
    "        print(f\"Sequence statistics:\")\n",
    "        print(f\"  Average length: {avg_length:.1f} amino acids\")\n",
    "        print(f\"  Minimum length: {min_length} amino acids\")\n",
    "        print(f\"  Maximum length: {max_length} amino acids\")\n",
    "        \n",
    "        # Print a few example sequences\n",
    "        print(\"\\nExample sequences from Unique genes file:\")\n",
    "        for i, record in enumerate(sequences):\n",
    "            if i < 3:  # Show first 3 sequences\n",
    "                print(f\">{record.id}\")\n",
    "                print(f\"  Length: {len(record.seq)} aa\")\n",
    "                print(f\"  Sequence: {record.seq[:50]}...\" if len(record.seq) > 50 else f\"  Sequence: {record.seq}\")\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to analyze sequences: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run annotation with less stringent E-value\n",
    "    try:\n",
    "        print(f\"\\nStarting annotation of {protein_file}...\")\n",
    "        print(f\"Using less stringent E-value threshold (1e-3) to increase sensitivity\")\n",
    "        \n",
    "        hmmer_output, annotations = annotate_unique_genes(\n",
    "            protein_file, \n",
    "            output_dir,\n",
    "            pfam_db=None,  # Auto-detect\n",
    "            cpu=4,\n",
    "            evalue=1e-3  # Less stringent threshold\n",
    "        )\n",
    "        \n",
    "        # Check if annotations were found\n",
    "        if not annotations:\n",
    "            print(\"\\nNo annotations found with E-value threshold of 1e-3.\")\n",
    "            print(\"Let's try with an even less stringent threshold (1e-2)...\")\n",
    "            \n",
    "            hmmer_output, annotations = annotate_unique_genes(\n",
    "                protein_file, \n",
    "                output_dir,\n",
    "                pfam_db=None,\n",
    "                cpu=4,\n",
    "                evalue=1e-2  # Very permissive threshold\n",
    "            )\n",
    "            \n",
    "        # Check HMMER output file content even if no annotations\n",
    "        if os.path.exists(hmmer_output) and os.path.getsize(hmmer_output) > 0:\n",
    "            print(\"\\nExamining raw HMMER output:\")\n",
    "            with open(hmmer_output, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                header_lines = [line for line in lines if line.startswith('#')]\n",
    "                data_lines = [line for line in lines if not line.startswith('#')]\n",
    "                \n",
    "                print(f\"  Header lines: {len(header_lines)}\")\n",
    "                print(f\"  Data lines: {len(data_lines)}\")\n",
    "                \n",
    "                if data_lines:\n",
    "                    print(\"\\nFirst few matches (if any):\")\n",
    "                    for i, line in enumerate(data_lines):\n",
    "                        if i < 3:\n",
    "                            print(f\"  {line.strip()}\")\n",
    "                        else:\n",
    "                            break\n",
    "                else:\n",
    "                    print(\"  No matches found in HMMER output.\")\n",
    "                    \n",
    "            # If still no annotations, try a different approach\n",
    "            if not annotations:\n",
    "                print(\"\\nNo Pfam domains found even with permissive threshold.\")\n",
    "                print(\"Suggestions:\")\n",
    "                print(\"1. Try a different domain database (e.g., TIGRFAMs, CDD)\")\n",
    "                print(\"2. Consider using sequence similarity search (BLAST) instead\")\n",
    "                print(\"3. Check if these sequences are likely to be known proteins\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Annotation failed: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Generate summary and visualizations\n",
    "    if annotations:\n",
    "        summary_file = summarize_annotations(annotations, output_dir, protein_file)\n",
    "        print(f\"Annotation summary saved to {summary_file}\")\n",
    "    else:\n",
    "        print(\"No annotations found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d912673-85dd-4b56-b65b-9906d4a87939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should see your result files in annotation_results/ subdirectory, files: NC_pfam_annotations.txt \n",
    "#.  and NC_pfam_parsed.csv\n",
    "# You will also see the output visualization graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29ef05-bc20-4452-82d4-65e2f244247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run into problems, you can try some of these types of prompts to fix / upgrade your code:\n",
    "# Write code to install the pram database for me\n",
    "# update the above code so the input file is in unique_genes/NC_unique_proteins.faa\n",
    "# It looks like there is an issue with the parsing function\n",
    "#. You can also paste any errors you get back into your AI tool and iterate until it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97795b5-e232-4b5f-acc5-122672c55a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Notebook ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
