{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a512488-4364-48dc-9d72-8dfd1e8749d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch16-2 Protein Design with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954b390-8bcf-4cb1-9412-b347d5f4ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProtGPT2 Protein Design - Jupyter Notebook\n",
    "# =============================================\n",
    "# This notebook demonstrates how to use ProtGPT2 to generate novel protein sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea4c46-9a55-41fa-8086-af767845e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages \n",
    "! pip install torch transformers numpy matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18781a-6883-4916-9f5c-867bf4187629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.  Import Libraries\n",
    "# =========================\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Optional\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80db7b-be9f-4e69-9ec2-ff1f6f71252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. COMPREHENSIVE WARNING SUPPRESSION\n",
    "# =================================\n",
    "# Set environment variable to suppress transformers warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "\n",
    "# Monkey patch the logging warning function to prevent specific padding warnings\n",
    "original_warning = warnings.warn\n",
    "def patched_warning(message, category=UserWarning, filename='', lineno=-1, file=None, line=None):\n",
    "    # Block specific padding warnings\n",
    "    if isinstance(message, str):\n",
    "        if any(phrase in message.lower() for phrase in [\n",
    "            'right-padding', 'padding_side', 'decoder-only', \n",
    "            'correct generation results', 'left when initializing'\n",
    "        ]):\n",
    "            return  # Don't show these warnings\n",
    "    # Show other warnings normally\n",
    "    original_warning(message, category, filename, lineno, file, line)\n",
    "\n",
    "# Apply the patch\n",
    "warnings.warn = patched_warning\n",
    "\n",
    "# Additional comprehensive warning filters\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "for logger_name in ['transformers', 'transformers.generation_utils', 'transformers.tokenization_utils']:\n",
    "    logging.getLogger(logger_name).setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8995c-8127-40df-a7fa-40281adc5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Print Header #\n",
    "print(\"‚úÖ All packages imported successfully!\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üíª CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7e941-74f9-403b-9394-5bac2a0ee3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. ProtGPT2 Designer Class\n",
    "# ===============================\n",
    "class ProtGPT2Designer:\n",
    "    \"\"\"A wrapper class for protein design using ProtGPT2.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"nferruz/ProtGPT2\"):\n",
    "        \"\"\"\n",
    "        Initialize the ProtGPT2 model and tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier for ProtGPT2\n",
    "        \"\"\"\n",
    "        print(\"üîÑ Loading ProtGPT2 model and tokenizer...\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"üñ•Ô∏è  Using device: {self.device}\")\n",
    "        \n",
    "        # Load model first\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load tokenizer with explicit configuration\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Configure tokenizer for decoder-only architecture\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        self.tokenizer.padding_side = 'left'\n",
    "        \n",
    "        # Configure model to match tokenizer\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.model.generation_config.eos_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "        print(f\"üìä Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "    \n",
    "    def generate_protein(self, \n",
    "                        prompt: str = \"<|endoftext|>\",\n",
    "                        max_length: int = 200,\n",
    "                        temperature: float = 1.0,\n",
    "                        num_sequences: int = 1,\n",
    "                        do_sample: bool = True,\n",
    "                        top_p: float = 0.9,\n",
    "                        show_progress: bool = True) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate protein sequences using ProtGPT2 (warning-free method).\n",
    "        \n",
    "        Args:\n",
    "            prompt: Starting sequence or prompt (use \"<|endoftext|>\" for de novo generation)\n",
    "            max_length: Maximum length of generated sequence\n",
    "            temperature: Sampling temperature (higher = more random)\n",
    "            num_sequences: Number of sequences to generate\n",
    "            do_sample: Whether to use sampling (vs greedy decoding)\n",
    "            top_p: Top-p sampling parameter\n",
    "            show_progress: Whether to show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            List of generated protein sequences\n",
    "        \"\"\"\n",
    "        if show_progress:\n",
    "            print(f\"üß¨ Generating {num_sequences} protein sequence(s)...\")\n",
    "        \n",
    "        sequences = []\n",
    "        \n",
    "        # Context manager to completely suppress output during generation\n",
    "        class SuppressOutput:\n",
    "            def __enter__(self):\n",
    "                self._original_stdout = sys.stdout\n",
    "                self._original_stderr = sys.stderr\n",
    "                sys.stdout = open(os.devnull, 'w')\n",
    "                sys.stderr = open(os.devnull, 'w')\n",
    "                return self\n",
    "            \n",
    "            def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "                sys.stdout.close()\n",
    "                sys.stderr.close()\n",
    "                sys.stdout = self._original_stdout\n",
    "                sys.stderr = self._original_stderr\n",
    "        \n",
    "        try:\n",
    "            for i in range(num_sequences):\n",
    "                # Tokenize input\n",
    "                input_ids = torch.tensor([self.tokenizer.encode(prompt)], device=self.device)\n",
    "                \n",
    "                # Generate with complete output suppression\n",
    "                with SuppressOutput():\n",
    "                    with torch.no_grad():\n",
    "                        output = self.model.generate(\n",
    "                            input_ids,\n",
    "                            max_length=max_length,\n",
    "                            temperature=temperature,\n",
    "                            num_return_sequences=1,\n",
    "                            do_sample=do_sample,\n",
    "                            top_p=top_p,\n",
    "                            pad_token_id=self.tokenizer.pad_token_id,\n",
    "                            eos_token_id=self.tokenizer.eos_token_id\n",
    "                        )\n",
    "                \n",
    "                # Decode the generated sequence\n",
    "                sequence = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Clean up the sequence\n",
    "                if prompt != \"<|endoftext|>\" and sequence.startswith(prompt):\n",
    "                    sequence = sequence[len(prompt):]\n",
    "                \n",
    "                sequences.append(sequence.strip())\n",
    "                \n",
    "        except Exception as e:\n",
    "            if show_progress:\n",
    "                print(f\"‚ùå Error in generation: {e}\")\n",
    "                print(\"üîÑ Trying fallback method...\")\n",
    "            \n",
    "            # Fallback to manual token generation\n",
    "            sequences = self._generate_manual_fallback(prompt, max_length, temperature, num_sequences)\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def _generate_manual_fallback(self, \n",
    "                                 prompt: str,\n",
    "                                 max_length: int,\n",
    "                                 temperature: float,\n",
    "                                 num_sequences: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Fallback generation method using manual token-by-token generation.\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for _ in range(num_sequences):\n",
    "            # Start with the prompt\n",
    "            current_text = prompt\n",
    "            \n",
    "            # Manual token-by-token generation\n",
    "            for _ in range(max_length):\n",
    "                # Encode current sequence\n",
    "                input_ids = torch.tensor([self.tokenizer.encode(current_text)], device=self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Get logits for next token\n",
    "                    outputs = self.model(input_ids)\n",
    "                    logits = outputs.logits[0, -1, :]  # Last token logits\n",
    "                    \n",
    "                    # Apply temperature\n",
    "                    if temperature != 1.0:\n",
    "                        logits = logits / temperature\n",
    "                    \n",
    "                    # Sample next token\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, 1).item()\n",
    "                    \n",
    "                    # Check for end of sequence\n",
    "                    if next_token == self.tokenizer.eos_token_id:\n",
    "                        break\n",
    "                    \n",
    "                    # Add token to sequence\n",
    "                    next_char = self.tokenizer.decode([next_token])\n",
    "                    current_text += next_char\n",
    "            \n",
    "            # Clean up the sequence\n",
    "            if prompt != \"<|endoftext|>\" and current_text.startswith(prompt):\n",
    "                sequence = current_text[len(prompt):]\n",
    "            else:\n",
    "                sequence = current_text.replace(\"<|endoftext|>\", \"\")\n",
    "            \n",
    "            sequences.append(sequence.strip())\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def validate_sequence(self, sequence: str) -> dict:\n",
    "        \"\"\"\n",
    "        Basic validation of a protein sequence.\n",
    "        \n",
    "        Args:\n",
    "            sequence: Protein sequence to validate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with validation results\n",
    "        \"\"\"\n",
    "        # Standard amino acids\n",
    "        standard_aa = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "        \n",
    "        # Clean sequence (remove whitespace and convert to uppercase)\n",
    "        clean_seq = re.sub(r'\\s+', '', sequence.upper())\n",
    "        \n",
    "        # Calculate statistics\n",
    "        length = len(clean_seq)\n",
    "        valid_aa = sum(1 for aa in clean_seq if aa in standard_aa)\n",
    "        invalid_aa = length - valid_aa\n",
    "        validity_ratio = valid_aa / length if length > 0 else 0\n",
    "        \n",
    "        # Calculate amino acid composition\n",
    "        aa_counts = {aa: clean_seq.count(aa) for aa in standard_aa}\n",
    "        aa_frequencies = {aa: count/length for aa, count in aa_counts.items() if length > 0}\n",
    "        \n",
    "        # Calculate some basic properties\n",
    "        hydrophobic_aa = set('AILMFPWV')\n",
    "        polar_aa = set('NQST')\n",
    "        charged_aa = set('DEKR')\n",
    "        aromatic_aa = set('FWY')\n",
    "        \n",
    "        hydrophobic_content = sum(aa_frequencies.get(aa, 0) for aa in hydrophobic_aa)\n",
    "        polar_content = sum(aa_frequencies.get(aa, 0) for aa in polar_aa)\n",
    "        charged_content = sum(aa_frequencies.get(aa, 0) for aa in charged_aa)\n",
    "        aromatic_content = sum(aa_frequencies.get(aa, 0) for aa in aromatic_aa)\n",
    "        \n",
    "        return {\n",
    "            'sequence': clean_seq,\n",
    "            'length': length,\n",
    "            'valid_amino_acids': valid_aa,\n",
    "            'invalid_amino_acids': invalid_aa,\n",
    "            'validity_ratio': validity_ratio,\n",
    "            'is_valid': validity_ratio > 0.95,\n",
    "            'amino_acid_composition': aa_frequencies,\n",
    "            'hydrophobic_content': hydrophobic_content,\n",
    "            'polar_content': polar_content,\n",
    "            'charged_content': charged_content,\n",
    "            'aromatic_content': aromatic_content\n",
    "        }\n",
    "    \n",
    "    def batch_generate_and_validate(self, \n",
    "                                   num_proteins: int = 10,\n",
    "                                   target_length: int = 150,\n",
    "                                   temperature: float = 0.8) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate multiple proteins and return validation results as DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            num_proteins: Number of proteins to generate\n",
    "            target_length: Target length for proteins\n",
    "            temperature: Sampling temperature\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with protein sequences and validation metrics\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(f\"üî¨ Generating and validating {num_proteins} proteins...\")\n",
    "        \n",
    "        for i in tqdm(range(num_proteins), desc=\"Generating proteins\"):\n",
    "            sequences = self.generate_protein(\n",
    "                max_length=target_length + 50,\n",
    "                temperature=temperature,\n",
    "                num_sequences=1,\n",
    "                show_progress=False\n",
    "            )\n",
    "            \n",
    "            # Ensure sequences is not None and is iterable\n",
    "            if sequences is None:\n",
    "                print(f\"‚ö†Ô∏è Warning: No sequences generated for protein {i+1}\")\n",
    "                continue\n",
    "                \n",
    "            for seq in sequences:\n",
    "                if seq:  # Make sure sequence is not empty\n",
    "                    validation = self.validate_sequence(seq)\n",
    "                    validation['protein_id'] = f\"protein_{i+1:03d}\"\n",
    "                    results.append(validation)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"‚ùå No valid proteins were generated!\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        df = pd.DataFrame(results)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c078893-a58b-4427-917d-242a89b06f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Initialize the Model\n",
    "# ============================\n",
    "print(\"üöÄ Initializing ProtGPT2 Designer...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "designer = ProtGPT2Designer()\n",
    "\n",
    "print(\"‚úÖ Ready to generate proteins!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a70e4-68b3-4f97-ae96-0911bf242809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Example 1 - de novo Protein Generation\n",
    "# ==============================================\n",
    "print(\"üß™ EXAMPLE 1: De novo protein generation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate some proteins from scratch\n",
    "sequences = designer.generate_protein(\n",
    "    prompt=\"<|endoftext|>\",\n",
    "    max_length=100,\n",
    "    temperature=0.8,\n",
    "    num_sequences=3\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i, seq in enumerate(sequences, 1):\n",
    "    print(f\"\\nüß¨ Generated Protein {i}:\")\n",
    "    print(f\"Sequence: {seq[:80]}{'...' if len(seq) > 80 else ''}\")\n",
    "    \n",
    "    # Validate the sequence\n",
    "    validation = designer.validate_sequence(seq)\n",
    "    print(f\"üìè Length: {validation['length']} amino acids\")\n",
    "    print(f\"‚úÖ Validity: {validation['validity_ratio']:.3f}\")\n",
    "    print(f\"üî¨ Valid: {'Yes' if validation['is_valid'] else 'No'}\")\n",
    "    print(f\"üíß Hydrophobic: {validation['hydrophobic_content']:.3f}\")\n",
    "    print(f\"‚ö° Charged: {validation['charged_content']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264a666-f6b7-40bc-9f03-03f34f77f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Example 2 - Motif-Based Generation\n",
    "# ==========================================\n",
    "print(\"\\nüéØ EXAMPLE 2: Protein generation with starting motif\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate proteins starting with a signal peptide-like motif\n",
    "motif = \"MKKLLF\"\n",
    "motif_sequences = designer.generate_protein(\n",
    "    prompt=motif,\n",
    "    max_length=120,\n",
    "    temperature=0.7,\n",
    "    num_sequences=2\n",
    ")\n",
    "\n",
    "for i, seq in enumerate(motif_sequences, 1):\n",
    "    full_sequence = motif + seq\n",
    "    print(f\"\\nüß¨ Motif-based Protein {i}:\")\n",
    "    print(f\"Sequence: {full_sequence[:80]}{'...' if len(full_sequence) > 80 else ''}\")\n",
    "    validation = designer.validate_sequence(full_sequence)\n",
    "    print(f\"üìè Length: {validation['length']} amino acids\")\n",
    "    print(f\"‚úÖ Validity: {validation['validity_ratio']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932cd6f8-f154-40da-b51e-91532ece86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18.  Batch Generation and Analysis\n",
    "# =====================================\n",
    "print(\"\\nüìä EXAMPLE 3: Batch generation and analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate multiple proteins for statistical analysis\n",
    "df_proteins = designer.batch_generate_and_validate(\n",
    "    num_proteins=20,\n",
    "    target_length=100,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(f\"\\nüìà Generated {len(df_proteins)} proteins\")\n",
    "print(f\"‚úÖ Valid proteins: {df_proteins['is_valid'].sum()}\")\n",
    "print(f\"üìè Average length: {df_proteins['length'].mean():.1f} ¬± {df_proteins['length'].std():.1f}\")\n",
    "print(f\"üéØ Validity ratio: {df_proteins['validity_ratio'].mean():.3f} ¬± {df_proteins['validity_ratio'].std():.3f}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "print(df_proteins[['length', 'validity_ratio', 'hydrophobic_content', \n",
    "                   'charged_content', 'polar_content', 'aromatic_content']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62cbe4e-7049-4ed8-86d0-28742ab1b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Visualization\n",
    "# ====================\n",
    "print(\"\\nüìà Creating visualizations...\")\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('ProtGPT2 Generated Protein Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Length distribution\n",
    "axes[0,0].hist(df_proteins['length'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].set_xlabel('Protein Length (amino acids)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('Length Distribution')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validity ratio distribution\n",
    "axes[0,1].hist(df_proteins['validity_ratio'], bins=15, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0,1].set_xlabel('Validity Ratio')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('Validity Ratio Distribution')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Amino acid composition comparison\n",
    "composition_data = df_proteins[['hydrophobic_content', 'charged_content', \n",
    "                               'polar_content', 'aromatic_content']]\n",
    "composition_means = composition_data.mean()\n",
    "axes[1,0].bar(range(len(composition_means)), composition_means.values, \n",
    "              color=['orange', 'red', 'blue', 'purple'], alpha=0.7)\n",
    "axes[1,0].set_xlabel('Amino Acid Type')\n",
    "axes[1,0].set_ylabel('Average Content')\n",
    "axes[1,0].set_title('Average Amino Acid Composition')\n",
    "axes[1,0].set_xticks(range(len(composition_means)))\n",
    "axes[1,0].set_xticklabels(['Hydrophobic', 'Charged', 'Polar', 'Aromatic'], rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Length vs validity scatter plot\n",
    "scatter = axes[1,1].scatter(df_proteins['length'], df_proteins['validity_ratio'], \n",
    "                           c=df_proteins['hydrophobic_content'], cmap='viridis', alpha=0.7)\n",
    "axes[1,1].set_xlabel('Protein Length')\n",
    "axes[1,1].set_ylabel('Validity Ratio')\n",
    "axes[1,1].set_title('Length vs Validity (colored by hydrophobic content)')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1,1], label='Hydrophobic Content')\n",
    "\n",
    "# Save Plot to file\n",
    "plt.savefig(\"ch16-2-protein-design.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00ce1b-f2f4-4c35-ae71-0dfa0776a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Display Sample Sequences\n",
    "# ================================\n",
    "print(\"\\nüß¨ Sample Generated Sequences:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show a few complete sequences\n",
    "valid_proteins = df_proteins[df_proteins['is_valid'] == True].head(5)\n",
    "\n",
    "for idx, row in valid_proteins.iterrows():\n",
    "    print(f\"\\n{row['protein_id'].upper()}:\")\n",
    "    print(f\"Sequence: {row['sequence']}\")\n",
    "    print(f\"Length: {row['length']} | Validity: {row['validity_ratio']:.3f} | \"\n",
    "          f\"Hydrophobic: {row['hydrophobic_content']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc71587-ba25-4f0b-a1bd-a2d06e0d40ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Temperature Comparison\n",
    "# ===============================\n",
    "print(\"\\n‚ö° ADVANCED EXAMPLE: Temperature comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compare different temperatures\n",
    "temperatures = [0.5, 0.8, 1.2]\n",
    "temp_results = {}\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nüå°Ô∏è  Temperature: {temp}\")\n",
    "    seqs = designer.generate_protein(\n",
    "        max_length=80,\n",
    "        temperature=temp,\n",
    "        num_sequences=2,\n",
    "        show_progress=False\n",
    "    )\n",
    "    \n",
    "    temp_results[temp] = seqs\n",
    "    for i, seq in enumerate(seqs, 1):\n",
    "        validation = designer.validate_sequence(seq)\n",
    "        print(f\"  Seq {i}: {seq[:50]}{'...' if len(seq) > 50 else ''} \"\n",
    "              f\"(L={validation['length']}, V={validation['validity_ratio']:.2f})\")\n",
    "\n",
    "print(\"\\nüéâ Analysis complete! You can now:\")\n",
    "print(\"   ‚Ä¢ Modify generation parameters in any cell\")\n",
    "print(\"   ‚Ä¢ Generate more proteins with different settings\")\n",
    "print(\"   ‚Ä¢ Analyze specific protein properties\")\n",
    "print(\"   ‚Ä¢ Export sequences for further analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d88354e-6a98-4fa6-bc43-01c6731612cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Optional: Save results\n",
    "# df_proteins.to_csv('generated_proteins.csv', index=False)\n",
    "# print(\"üíæ Results saved to 'generated_proteins.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bfb56-c4bd-4bbf-8ac9-5a27c9d7fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Notebook ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
