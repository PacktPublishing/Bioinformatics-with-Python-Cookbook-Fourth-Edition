{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1a6ac-8fab-4798-b7d7-b6847db9bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch06-1 Machine Learning for BioInformatics with PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd6ca11-9a91-4c31-a8e0-048612376467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNV Analysis - Explore Copy Number Variation with CNNs #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9293ea-b6ca-4df8-b69e-ee3d953e7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd51df-80f8-4d3b-b52c-2199bacd8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries # \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a65da2f-5d12-4c3c-8efb-c764e8fa9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103fa292-9b97-45eb-adee-1c46bf877da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.  Class for Simulated CNV Data Generation\n",
    "class CNVDataGenerator:\n",
    "    \"\"\"\n",
    "    Generate simulated copy number variation data for training.\n",
    "    \n",
    "    Simulates genomic bins with coverage data and introduces CNVs\n",
    "    (deletions and duplications) at various locations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_samples=1000, sequence_length=200, bin_size=1000):\n",
    "        self.n_samples = n_samples\n",
    "        self.sequence_length = sequence_length  # Number of genomic bins\n",
    "        self.bin_size = bin_size  # Size of each bin in bp\n",
    "        \n",
    "    def generate_normal_coverage(self, mean_coverage=30, noise_std=5):\n",
    "        \"\"\"Generate baseline coverage with realistic noise patterns.\"\"\"\n",
    "        # Base coverage with some regional variation\n",
    "        base_coverage = np.random.normal(mean_coverage, noise_std/2, self.sequence_length)\n",
    "        \n",
    "        # Add GC content bias (some regions have higher/lower coverage)\n",
    "        gc_bias = np.sin(np.linspace(0, 4*np.pi, self.sequence_length)) * 3\n",
    "        base_coverage += gc_bias\n",
    "        \n",
    "        # Add random noise\n",
    "        noise = np.random.normal(0, noise_std, self.sequence_length)\n",
    "        coverage = base_coverage + noise\n",
    "        \n",
    "        # Ensure non-negative coverage\n",
    "        coverage = np.maximum(coverage, 1)\n",
    "        \n",
    "        return coverage\n",
    "    \n",
    "    def introduce_cnv(self, coverage, cnv_type, start_pos, end_pos, intensity=0.5):\n",
    "        \"\"\"\n",
    "        Introduce a copy number variation into the coverage data.\n",
    "        \n",
    "        Args:\n",
    "            coverage: Base coverage array\n",
    "            cnv_type: 'deletion' or 'duplication'\n",
    "            start_pos: Start position of CNV\n",
    "            end_pos: End position of CNV\n",
    "            intensity: Strength of the CNV (0.0 to 1.0)\n",
    "        \"\"\"\n",
    "        cnv_coverage = coverage.copy()\n",
    "        \n",
    "        if cnv_type == 'deletion':\n",
    "            # Reduce coverage in the CNV region\n",
    "            reduction_factor = 0.1 + (1 - intensity) * 0.4  # 0.1 to 0.5\n",
    "            cnv_coverage[start_pos:end_pos] *= reduction_factor\n",
    "            \n",
    "        elif cnv_type == 'duplication':\n",
    "            # Increase coverage in the CNV region\n",
    "            amplification_factor = 1.5 + intensity * 1.0  # 1.5 to 2.5\n",
    "            cnv_coverage[start_pos:end_pos] *= amplification_factor\n",
    "            \n",
    "        return cnv_coverage\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        \"\"\"Generate complete dataset with CNVs and labels.\"\"\"\n",
    "        X = []  # Coverage data\n",
    "        y = []  # Labels (0=normal, 1=deletion, 2=duplication)\n",
    "        cnv_positions = []  # For visualization\n",
    "        \n",
    "        for i in range(self.n_samples):\n",
    "            # Generate base coverage\n",
    "            coverage = self.generate_normal_coverage()\n",
    "            \n",
    "            # Create label array (initially all normal)\n",
    "            labels = np.zeros(self.sequence_length, dtype=int)\n",
    "            \n",
    "            # Randomly decide if this sample has CNVs\n",
    "            has_cnv = np.random.random() > 0.3  # 70% chance of having CNV\n",
    "            \n",
    "            sample_cnvs = []\n",
    "            \n",
    "            if has_cnv:\n",
    "                # Add 1-3 CNVs to this sample\n",
    "                n_cnvs = np.random.randint(1, 4)\n",
    "                \n",
    "                for _ in range(n_cnvs):\n",
    "                    # Random CNV parameters\n",
    "                    cnv_length = np.random.randint(10, 50)  # CNV length in bins\n",
    "                    start_pos = np.random.randint(0, self.sequence_length - cnv_length)\n",
    "                    end_pos = start_pos + cnv_length\n",
    "                    \n",
    "                    cnv_type = np.random.choice(['deletion', 'duplication'])\n",
    "                    intensity = np.random.uniform(0.3, 0.8)\n",
    "                    \n",
    "                    # Apply CNV to coverage\n",
    "                    coverage = self.introduce_cnv(coverage, cnv_type, start_pos, end_pos, intensity)\n",
    "                    \n",
    "                    # Update labels\n",
    "                    if cnv_type == 'deletion':\n",
    "                        labels[start_pos:end_pos] = 1\n",
    "                    else:  # duplication\n",
    "                        labels[start_pos:end_pos] = 2\n",
    "                    \n",
    "                    sample_cnvs.append({\n",
    "                        'type': cnv_type,\n",
    "                        'start': start_pos,\n",
    "                        'end': end_pos,\n",
    "                        'intensity': intensity\n",
    "                    })\n",
    "            \n",
    "            X.append(coverage)\n",
    "            y.append(labels)\n",
    "            cnv_positions.append(sample_cnvs)\n",
    "        \n",
    "        return np.array(X), np.array(y), cnv_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc696db-3e1a-43fe-8ff7-c051dcbe76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.  Class to hold CNV datasets\n",
    "class CNVDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for CNV detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f15a46-6558-4e3d-91f6-5b4a46e91b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.  CNV Detector class using Convolutional Neural Network (CNN)\n",
    "class CNVDetectorCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for Copy Number Variation detection.\n",
    "    \n",
    "    Architecture:\n",
    "    - 1D Convolutions to capture local patterns in coverage\n",
    "    - Batch normalization and dropout for regularization\n",
    "    - Multi-scale feature extraction with different kernel sizes\n",
    "    - Classification head for 3-class prediction (normal, deletion, duplication)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=200, n_classes=3):\n",
    "        super(CNVDetectorCNN, self).__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Multi-scale convolutional layers\n",
    "        # Small kernels for local patterns\n",
    "        self.conv1a = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv1b = nn.Conv1d(1, 64, kernel_size=5, padding=2)\n",
    "        self.conv1c = nn.Conv1d(1, 64, kernel_size=7, padding=3)\n",
    "        \n",
    "        # Combine multi-scale features\n",
    "        self.conv2 = nn.Conv1d(192, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        \n",
    "        # Global features\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Classification layers\n",
    "        self.fc1 = nn.Linear(256 + sequence_length, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, n_classes * sequence_length)\n",
    "        \n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Input shape: (batch_size, sequence_length)\n",
    "        # Reshape for 1D convolution: (batch_size, 1, sequence_length)\n",
    "        x_conv = x.unsqueeze(1)\n",
    "        \n",
    "        # Multi-scale convolutions\n",
    "        conv1a_out = F.relu(self.conv1a(x_conv))\n",
    "        conv1b_out = F.relu(self.conv1b(x_conv))\n",
    "        conv1c_out = F.relu(self.conv1c(x_conv))\n",
    "        \n",
    "        # Concatenate multi-scale features\n",
    "        x_conv = torch.cat([conv1a_out, conv1b_out, conv1c_out], dim=1)\n",
    "        x_conv = self.bn1(x_conv)\n",
    "        x_conv = self.dropout1(x_conv)\n",
    "        \n",
    "        # Additional convolutional layers\n",
    "        x_conv = F.relu(self.conv2(x_conv))\n",
    "        x_conv = self.bn2(x_conv)\n",
    "        x_conv = self.dropout2(x_conv)\n",
    "        \n",
    "        x_conv = F.relu(self.conv3(x_conv))\n",
    "        x_conv = self.bn3(x_conv)\n",
    "        x_conv = self.dropout3(x_conv)\n",
    "        \n",
    "        # Global pooling for global features\n",
    "        global_features = self.global_pool(x_conv).squeeze(-1)\n",
    "        \n",
    "        # Combine original signal with global features\n",
    "        combined = torch.cat([global_features, x], dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x_fc = F.relu(self.fc1(combined))\n",
    "        x_fc = self.dropout_fc(x_fc)\n",
    "        x_fc = F.relu(self.fc2(x_fc))\n",
    "        x_fc = self.dropout_fc(x_fc)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.fc3(x_fc)\n",
    "        \n",
    "        # Reshape to (batch_size, sequence_length, n_classes)\n",
    "        output = output.view(batch_size, self.sequence_length, self.n_classes)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515e660-12dd-4859-bf59-1ea4be8e99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.  CNV training and evaluation class\n",
    "class CNVTrainer:\n",
    "    \"\"\"Training and evaluation pipeline for CNV detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "    def train_epoch(self, train_loader, optimizer, criterion):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(batch_x)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            outputs = outputs.view(-1, 3)  # (batch_size * seq_length, n_classes)\n",
    "            batch_y = batch_y.view(-1)     # (batch_size * seq_length)\n",
    "            \n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate(self, val_loader, criterion):\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                outputs = self.model(batch_x)\n",
    "                \n",
    "                # Reshape for loss calculation\n",
    "                outputs = outputs.view(-1, 3)\n",
    "                batch_y = batch_y.view(-1)\n",
    "                \n",
    "                loss = criterion(outputs, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=50, lr=0.001):\n",
    "        \"\"\"Complete training loop.\"\"\"\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader, optimizer, criterion)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate(val_loader, criterion)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Store metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch:3d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                      f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485bd80-116e-44b0-9fe0-c24dd3b9e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.  Function to visualize network architecture\n",
    "def visualize_network_architecture(model):\n",
    "    \"\"\"Visualize the neural network architecture.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Network structure visualization\n",
    "    layers = [\n",
    "        \"Input\\n(Coverage Data)\\n[200 bins]\",\n",
    "        \"Multi-scale Conv1D\\n(3x3, 5x5, 7x7)\\n[64 filters each]\",\n",
    "        \"Concat + BatchNorm\\n[192 channels]\",\n",
    "        \"Conv1D + BatchNorm\\n[128 channels]\", \n",
    "        \"Conv1D + BatchNorm\\n[256 channels]\",\n",
    "        \"Global Pool +\\nOriginal Signal\\n[256 + 200]\",\n",
    "        \"FC Layer\\n[512 neurons]\",\n",
    "        \"FC Layer\\n[256 neurons]\",\n",
    "        \"Output\\n[3 classes per bin]\\n[Normal, Del, Dup]\"\n",
    "    ]\n",
    "    \n",
    "    # Draw network layers\n",
    "    y_positions = np.linspace(0.9, 0.1, len(layers))\n",
    "    \n",
    "    for i, (layer, y_pos) in enumerate(zip(layers, y_positions)):\n",
    "        # Layer box\n",
    "        box_width = 0.15\n",
    "        box_height = 0.08\n",
    "        \n",
    "        if i == 0:  # Input layer\n",
    "            color = 'lightblue'\n",
    "        elif i < 5:  # Convolutional layers\n",
    "            color = 'lightgreen'\n",
    "        elif i < 7:  # Fully connected layers\n",
    "            color = 'lightyellow'\n",
    "        else:  # Output layer\n",
    "            color = 'lightcoral'\n",
    "            \n",
    "        rect = plt.Rectangle((0.4, y_pos - box_height/2), box_width, box_height,\n",
    "                           facecolor=color, edgecolor='black', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Layer text\n",
    "        ax.text(0.475, y_pos, layer, ha='center', va='center', fontsize=9, weight='bold')\n",
    "        \n",
    "        # Arrows between layers\n",
    "        if i < len(layers) - 1:\n",
    "            ax.arrow(0.475, y_pos - box_height/2 - 0.01, 0, -0.04, \n",
    "                    head_width=0.01, head_length=0.01, fc='black', ec='black')\n",
    "    \n",
    "    # Add side annotations\n",
    "    ax.text(0.05, 0.7, \"Feature\\nExtraction\", ha='center', va='center', \n",
    "            fontsize=12, weight='bold', rotation=90)\n",
    "    ax.text(0.05, 0.3, \"Classification\", ha='center', va='center', \n",
    "            fontsize=12, weight='bold', rotation=90)\n",
    "    \n",
    "    ax.set_xlim(0, 0.8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('CNV Detector Neural Network Architecture', fontsize=16, weight='bold', pad=20)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightblue', label='Input'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightgreen', label='Convolutional'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightyellow', label='Fully Connected'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightcoral', label='Output')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.95, 0.95))\n",
    "    # Save Plot to file\n",
    "    plt.savefig(\"ch06-1-network-architecture.png\", dpi=300, bbox_inches='tight')\n",
    "    # Display the Plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f61fcc-a0b4-4c07-a72c-dfc897d949a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Function to visualize the training results\n",
    "def visualize_training_results(trainer):\n",
    "    \"\"\"Visualize training progress.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(trainer.train_losses) + 1)\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0].plot(epochs, trainer.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, trainer.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    axes[1].plot(epochs, trainer.train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "    axes[1].plot(epochs, trainer.val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save Plot to file\n",
    "    plt.savefig(\"ch06-1-training-results.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8033ca3-9666-4917-a46c-8fd0257a0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.  Visualize CVN predictions\n",
    "def visualize_cnv_predictions(model, test_data, test_labels, cnv_positions, n_samples=3):\n",
    "    \"\"\"Visualize CNV predictions on test samples.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 1, figsize=(15, 4*n_samples))\n",
    "    if n_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    class_names = ['Normal', 'Deletion', 'Duplication']\n",
    "    colors = ['gray', 'blue', 'red']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(n_samples):\n",
    "            # Get prediction\n",
    "            sample_input = torch.FloatTensor(test_data[i:i+1])\n",
    "            output = model(sample_input)\n",
    "            predictions = torch.argmax(output, dim=2).squeeze().numpy()\n",
    "            \n",
    "            # Plot coverage data\n",
    "            positions = np.arange(len(test_data[i]))\n",
    "            axes[i].plot(positions, test_data[i], 'k-', alpha=0.7, linewidth=1, label='Coverage')\n",
    "            \n",
    "            # Plot true CNVs\n",
    "            true_labels = test_labels[i]\n",
    "            for class_idx in [1, 2]:  # Deletion and duplication\n",
    "                mask = true_labels == class_idx\n",
    "                if np.any(mask):\n",
    "                    axes[i].fill_between(positions, 0, np.max(test_data[i]), \n",
    "                                       where=mask, alpha=0.3, color=colors[class_idx], \n",
    "                                       label=f'True {class_names[class_idx]}')\n",
    "            \n",
    "            # Plot predicted CNVs\n",
    "            for class_idx in [1, 2]:  # Deletion and duplication\n",
    "                mask = predictions == class_idx\n",
    "                if np.any(mask):\n",
    "                    # Plot as markers above the coverage\n",
    "                    pred_positions = positions[mask]\n",
    "                    pred_heights = np.max(test_data[i]) * 1.1\n",
    "                    axes[i].scatter(pred_positions, [pred_heights] * len(pred_positions),\n",
    "                                  marker='v' if class_idx == 1 else '^', \n",
    "                                  color=colors[class_idx], s=20, alpha=0.8,\n",
    "                                  label=f'Pred {class_names[class_idx]}')\n",
    "            \n",
    "            axes[i].set_xlabel('Genomic Position (bins)')\n",
    "            axes[i].set_ylabel('Coverage')\n",
    "            axes[i].set_title(f'Sample {i+1}: CNV Detection Results')\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "    # Save Plot to file\n",
    "    plt.savefig(\"ch06-1-cnv-results.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Display the Plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91df034-10c1-4576-b0e5-c574755d8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.  Main code section to run CNV analysis\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=== CNV Detection with PyTorch ===\\n\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # Generate synthetic CNV data\n",
    "    print(\"1. Generating synthetic CNV data...\")\n",
    "    data_generator = CNVDataGenerator(n_samples=800, sequence_length=200)\n",
    "    X, y, cnv_positions = data_generator.generate_dataset()\n",
    "    \n",
    "    print(f\"Generated {X.shape[0]} samples with {X.shape[1]} genomic bins each\")\n",
    "    print(f\"Class distribution:\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for class_idx, count in zip(unique, counts):\n",
    "        class_names = ['Normal', 'Deletion', 'Duplication']\n",
    "        print(f\"  {class_names[class_idx]}: {count:,} bins ({count/np.prod(y.shape)*100:.1f}%)\")\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.7 * len(X))\n",
    "    val_size = int(0.15 * len(X))\n",
    "    \n",
    "    X_train, y_train = X[:train_size], y[:train_size]\n",
    "    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "    X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "    cnv_test = cnv_positions[train_size+val_size:]\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = CNVDataset(X_train, y_train)\n",
    "    val_dataset = CNVDataset(X_val, y_val)\n",
    "    test_dataset = CNVDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"  Training: {len(train_dataset)} samples\")\n",
    "    print(f\"  Validation: {len(val_dataset)} samples\") \n",
    "    print(f\"  Test: {len(test_dataset)} samples\")\n",
    "    \n",
    "    # Create and visualize model\n",
    "    print(\"\\n2. Creating neural network model...\")\n",
    "    model = CNVDetectorCNN(sequence_length=200, n_classes=3)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model created with {total_params:,} total parameters ({trainable_params:,} trainable)\")\n",
    "    \n",
    "    # Visualize network architecture\n",
    "    print(\"\\n3. Visualizing network architecture...\")\n",
    "    visualize_network_architecture(model)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\n4. Training the model...\")\n",
    "    trainer = CNVTrainer(model, device)\n",
    "    trainer.train(train_loader, val_loader, epochs=30, lr=0.001)\n",
    "    \n",
    "    # Visualize training results\n",
    "    print(\"\\n5. Visualizing training results...\")\n",
    "    visualize_training_results(trainer)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n6. Evaluating on test set...\")\n",
    "    test_loss, test_accuracy = trainer.validate(test_loader, nn.CrossEntropyLoss())\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Generate detailed classification report\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            predictions = torch.argmax(outputs, dim=2)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy().flatten())\n",
    "            all_targets.extend(batch_y.numpy().flatten())\n",
    "    \n",
    "    # Classification report\n",
    "    class_names = ['Normal', 'Deletion', 'Duplication']\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(all_targets, all_predictions, target_names=class_names))\n",
    "    \n",
    "    # Visualize predictions\n",
    "    print(\"\\n7. Visualizing CNV detection results...\")\n",
    "    visualize_cnv_predictions(model, X_test, y_test, cnv_test, n_samples=3)\n",
    "    \n",
    "    print(\"\\n=== Analysis Complete ===\")\n",
    "    \n",
    "    return model, trainer, (X_test, y_test, cnv_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, trainer, test_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923b11e-70d2-4a95-8347-377d1aee0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End of Notebook ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
